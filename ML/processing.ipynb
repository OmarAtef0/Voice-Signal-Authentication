{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from sklearn.metrics import accuracy_score\n",
    "import librosa\n",
    "import resampy\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define features extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_feature_extractor(audio,sampleRate):\n",
    "    mfccsFeatures = librosa.feature.mfcc(y=audio, sr=sampleRate, n_mfcc=40)\n",
    "    mfccsScaledFeatures = np.mean(mfccsFeatures.T,axis=0)\n",
    "    \n",
    "    return mfccsScaledFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_feature_extractor(audio,sampleRate):\n",
    "    stft = np.abs(librosa.stft(audio))\n",
    "    contrast = librosa.feature.spectral_contrast(S=stft, sr=sampleRate) \n",
    "    contrastScaled = np.mean(contrast.T,axis=0)\n",
    "    \n",
    "    return contrastScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tonnetz_feature_extractor(audio,sampleRate):\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio), sr=sampleRate)\n",
    "    tonnetzScaled = np.mean(tonnetz.T,axis=0)\n",
    "    \n",
    "    return tonnetzScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_feature_extractor(audio,sampleRate):\n",
    "    centroid = librosa.feature.spectral_centroid(y=audio,sr=sampleRate) \n",
    "    centroidScaled = np.mean(centroid.T,axis=0)\n",
    "    \n",
    "    return centroidScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_feature_extractor(audio,sampleRate):\n",
    "    stft = np.abs(librosa.stft(audio))\n",
    "    chroma = librosa.feature.chroma_stft(S=stft,sr=sampleRate)\n",
    "    chromaScaled = np.mean(chroma.T,axis=0)\n",
    "    \n",
    "    return chromaScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    if file.lower().endswith(\".m4a\"):\n",
    "        audio = AudioSegment.from_file(file, format=\"m4a\")\n",
    "        file_name, _ = os.path.splitext(os.path.basename(file))\n",
    "        output_path = os.path.join(os.path.dirname(file), file_name + \".wav\")\n",
    "        audio.export(output_path, format=\"wav\")\n",
    "        # os.remove(file)\n",
    "        file = output_path\n",
    "\n",
    "    features = []\n",
    "    audio, sample_rate = librosa.load(file, res_type='kaiser_fast') \n",
    "    mfcc = mfcc_feature_extractor(audio, sample_rate)\n",
    "    contrast = contrast_feature_extractor(audio, sample_rate)\n",
    "    tonnetz = tonnetz_feature_extractor(audio, sample_rate)\n",
    "    chroma = chroma_feature_extractor(audio, sample_rate)\n",
    "\n",
    "    # Concatenate individual feature arrays\n",
    "    concatenated_features = np.concatenate((mfcc, contrast, tonnetz, chroma), axis=0)\n",
    "    features.append(concatenated_features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define add features function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(extractedFeatures, dirPath, label):\n",
    "    for file in os.listdir(dirPath):\n",
    "        filePath = os.path.join(dirPath, file)\n",
    "\n",
    "        if filePath.lower().endswith(\".m4a\"):\n",
    "            audio = AudioSegment.from_file(filePath, format=\"m4a\")\n",
    "            file_name, _ = os.path.splitext(os.path.basename(filePath))\n",
    "            output_path = os.path.join(os.path.dirname(filePath), file_name + \".wav\")\n",
    "            audio.export(output_path, format=\"wav\")\n",
    "            # os.remove(filePath)\n",
    "            filePath = output_path\n",
    "\n",
    "        audio, sampleRate = librosa.load(filePath, res_type='kaiser_fast') \n",
    "        mfcc = mfcc_feature_extractor(audio, sampleRate)\n",
    "        contrast = contrast_feature_extractor(audio, sampleRate)\n",
    "        tonnetz = tonnetz_feature_extractor(audio, sampleRate)\n",
    "        chroma = chroma_feature_extractor(audio, sampleRate)\n",
    "\n",
    "        extractedFeatures.append([mfcc, contrast, tonnetz, chroma, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dictionary with dirPath and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {\n",
    "    \"ibrahim\": [\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/ibrahim\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/ibrahim\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/grant/ibrahim\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/ibrahim\",\n",
    "    ],\n",
    "    \"ahmeda\": [\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/ahmeda\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/ahmeda\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/grant/ahmeda\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/ahmeda\",\n",
    "    ],\n",
    "    \"omar\": [\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/omar\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/omar\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/grant/omar\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/omar\",\n",
    "    ],\n",
    "    \"hazem\": [\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/hazem\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/hazem\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/grant/hazem\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/hazem\",\n",
    "    ],\n",
    "    \"ahmedk\": [\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/ahmedk\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/ahmedk\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/grant/ahmedk\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/ahmedk\",\n",
    "    ],\n",
    "    \"hassan\": [\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/hassan\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/hassan\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/grant/hassan\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/hassan\",\n",
    "    ],\n",
    "    \"mohannad\": [\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/mohannad\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/mohannad\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/grant/mohannad\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/mohannad\",\n",
    "    ],\n",
    "    \"other\":[\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other people\", \n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/tamer\", \n",
    "    ],\n",
    "    \"abdelrahman\": [\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/abdelrahman\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/abdelrahman\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/grant/abdelrahman\",\n",
    "        \"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/abdelrahman\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating data frame from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractedFeatures = []\n",
    "for label in dict1.keys():\n",
    "    for link in dict1[label]:\n",
    "        add_features(extractedFeatures, link, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mfcc</th>\n",
       "      <td>614</td>\n",
       "      <td>614</td>\n",
       "      <td>[-516.205, 153.0361, -8.050304, 33.207005, 1.4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>614</td>\n",
       "      <td>614</td>\n",
       "      <td>[21.502896924279796, 15.580563903506814, 14.88...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tonnetz</th>\n",
       "      <td>614</td>\n",
       "      <td>614</td>\n",
       "      <td>[0.05704616298945937, 0.08094122501686858, 0.1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chroma</th>\n",
       "      <td>614</td>\n",
       "      <td>614</td>\n",
       "      <td>[0.48283857, 0.58331424, 0.63629895, 0.4935803...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>614</td>\n",
       "      <td>9</td>\n",
       "      <td>other</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count unique                                                top freq\n",
       "mfcc       614    614  [-516.205, 153.0361, -8.050304, 33.207005, 1.4...    1\n",
       "contrast   614    614  [21.502896924279796, 15.580563903506814, 14.88...    1\n",
       "tonnetz    614    614  [0.05704616298945937, 0.08094122501686858, 0.1...    1\n",
       "chroma     614    614  [0.48283857, 0.58331424, 0.63629895, 0.4935803...    1\n",
       "class      614      9                                              other  169"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(extractedFeatures, columns=['mfcc','contrast','tonnetz','chroma','class'])\n",
    "data.to_csv('processing.csv')\n",
    "print(data.shape)\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ibrahim', 'ahmeda', 'omar', 'hazem', 'ahmedk', 'hassan',\n",
       "       'mohannad', 'other', 'abdelrahman'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data.iloc[:,0:-1]\n",
    "target = data['class']\n",
    "target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.values.tolist()\n",
    "for i in range(len(features)):\n",
    "    features[i] = np.concatenate((features[i][0],features[i][1],features[i][2],features[i][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "target = encoder.fit_transform(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(features, target, test_size=0.25, random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    45\n",
      "1    34\n",
      "2    60\n",
      "3    32\n",
      "4    79\n",
      "5    80\n",
      "6    33\n",
      "7    82\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,8,1):\n",
    "  print(i,\"  \", len(target[target==i]))\n",
    "  # {'ahmeda': 0, 'ahmedk': 1, 'hassan': 2, 'hazem': 3, 'ibrahim': 4, 'mohannad': 5, 'omar': 6, 'other': 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = SVC(kernel='linear',decision_function_shape=\"ovo\" ,probability=True) \n",
    "# classifier.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_class=8, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_class=8, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_class=8, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = XGBClassifier(objective='multi:softmax', num_class=8, eval_metric='mlogloss')\n",
    "classifier.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "# classifier.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicting xTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predX = classifier.predict(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making classification report to see the result of the predicton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       0.86      1.00      0.92        12\n",
      "           2       1.00      1.00      1.00        13\n",
      "           3       0.86      1.00      0.92         6\n",
      "           4       0.94      0.94      0.94        17\n",
      "           5       0.90      0.95      0.93        20\n",
      "           6       1.00      0.70      0.82        10\n",
      "           7       0.94      1.00      0.97        17\n",
      "           8       1.00      0.98      0.99        50\n",
      "\n",
      "    accuracy                           0.95       154\n",
      "   macro avg       0.94      0.94      0.94       154\n",
      "weighted avg       0.96      0.95      0.95       154\n",
      "\n",
      "1.0    0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yTest, predX))\n",
    "\n",
    "train_predictions = classifier.predict(xTrain)\n",
    "test_predictions = classifier.predict(xTest)\n",
    "\n",
    "train_accuracy = accuracy_score(yTrain, train_predictions)\n",
    "test_accuracy = accuracy_score(yTest, test_predictions)\n",
    "\n",
    "print(train_accuracy, \"  \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(pred):   \n",
    "    # {'ahmeda': 0, 'ahmedk': 1, 'hassan': 2, 'hazem': 3, 'ibrahim': 4, 'mohannad': 5, 'omar': 6, 'other': 7}\n",
    "    if pred == 0:\n",
    "        return (\"abdelrahman\")\n",
    "    elif pred == 1:\n",
    "        return (\"ahmeda\")\n",
    "    elif pred == 2:\n",
    "        return (\"ahmedk\")\n",
    "    elif pred == 3:\n",
    "        return (\"hassan\")\n",
    "    elif pred == 4:\n",
    "        return (\"hazem\")\n",
    "    elif pred == 5:\n",
    "        return (\"ibrahim\")\n",
    "    elif pred == 6:\n",
    "        return (\"mohannad\")\n",
    "    elif pred == 7:\n",
    "        return (\"omar\")\n",
    "    elif pred == 8:\n",
    "        return (\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(features):\n",
    "    probabilities = classifier.predict_proba(features)\n",
    "    # print(probabilities)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        class_pred_index = probabilities[i].argmax()  \n",
    "        class_pred = probabilities[i, class_pred_index]  \n",
    "        threshold = 0.75\n",
    "        \n",
    "        if class_pred > threshold:\n",
    "            return class_pred_index\n",
    "        else:\n",
    "            return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4149844e-04 2.7226814e-04 9.9621528e-01 3.8169583e-04 4.2453525e-04\n",
      "  3.6370009e-04 2.8607543e-04 4.6650853e-04 1.1485552e-03]]\n",
      "[2]\n",
      "1:  ahmedk\n",
      "2:  ahmedk\n",
      "[[1.0991342e-03 6.7782705e-04 9.9183196e-01 7.0229330e-04 1.4033066e-03\n",
      "  7.5375725e-04 7.1220053e-04 9.0374728e-04 1.9157856e-03]]\n",
      "[2]\n",
      "1:  ahmedk\n",
      "2:  ahmedk\n",
      "[[5.9992290e-04 3.6996731e-04 9.9526608e-01 3.8332134e-04 6.3969620e-04\n",
      "  4.4833357e-04 3.8872892e-04 6.3390745e-04 1.2700156e-03]]\n",
      "[2]\n",
      "1:  ahmedk\n",
      "2:  ahmedk\n",
      "[[3.9731080e-04 2.4501828e-04 9.9665987e-01 2.5386215e-04 3.2410931e-04\n",
      "  5.2853557e-04 2.5744367e-04 4.4698021e-04 8.8697480e-04]]\n",
      "[2]\n",
      "1:  ahmedk\n",
      "2:  ahmedk\n",
      "[[3.1940872e-04 2.7693395e-04 9.9374145e-01 2.8693004e-04 3.4587385e-04\n",
      "  5.1744533e-04 2.9097783e-04 5.5828242e-04 3.6626994e-03]]\n",
      "[2]\n",
      "1:  ahmedk\n",
      "2:  ahmedk\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/ahmedk\")[0:5]:\n",
    "  testFeatures = features_extractor(f\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/open/ahmedk/{file}\") \n",
    "  print(classifier.predict_proba(testFeatures))\n",
    "  print(\"1: \",prediction(classifier.predict(testFeatures)))\n",
    "  print(\"2: \",prediction(my_predict(testFeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.7488594  1.58241    3.8227704  2.0380929  4.7362075  3.8146112\n",
      "   1.3866773 78.27659    1.5937811]]\n",
      "1:  omar\n",
      "2:  omar\n",
      "[[ 2.7488594  1.58241    3.8227704  2.0380929  4.7362075  3.8146112\n",
      "   1.3866773 78.27659    1.5937811]]\n",
      "1:  omar\n",
      "2:  omar\n",
      "[[ 6.9359245  2.8084505  4.895044   1.6950024  4.728669   4.0906725\n",
      "   1.9899137 70.01802    2.8383102]]\n",
      "1:  omar\n",
      "2:  omar\n",
      "[[ 6.9359245  2.8084505  4.895044   1.6950024  4.728669   4.0906725\n",
      "   1.9899137 70.01802    2.8383102]]\n",
      "1:  omar\n",
      "2:  omar\n",
      "[[ 0.97868246  0.16720338  0.50934976  0.3619991   0.17993034  0.27745456\n",
      "   0.20843662 97.05155     0.26539516]]\n",
      "1:  omar\n",
      "2:  omar\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/omar\")[0:5]:\n",
    "  testFeatures = features_extractor(f\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/unlock/omar/{file}\") \n",
    "  print(classifier.predict_proba(testFeatures)*100)\n",
    "  print(\"1: \",prediction(classifier.predict(testFeatures)))\n",
    "  print(\"2: \",prediction(my_predict(testFeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording (10)_638396660155783233_wav.wav\n",
      "[[2.6662689e-02 5.7343412e-02 4.3690387e-02 9.0203173e-02 9.7874443e+01\n",
      "  8.6081259e-02 3.2052826e-02 6.0522139e-01 1.1842983e+00]]\n",
      "1:  hazem\n",
      "2:  hazem\n",
      "Recording (10)_638397160016988114_wav.wav\n",
      "[[ 0.40198332  0.34320405  0.18178555  0.14250684 98.31476     0.09997299\n",
      "   0.13540852  0.15448573  0.22589397]]\n",
      "1:  hazem\n",
      "2:  hazem\n",
      "Recording (11)_638396660155986766_wav.wav\n",
      "[[ 0.9058727   0.2394497   0.23543239  0.17220719 93.50446     0.14909138\n",
      "   0.17272149  1.095186    3.5255744 ]]\n",
      "1:  hazem\n",
      "2:  hazem\n",
      "Recording (11)_638397160017160202_wav.wav\n",
      "[[2.12465972e-01 1.74694493e-01 1.11789025e-01 9.84428450e-02\n",
      "  9.85966644e+01 4.78911400e-01 1.56728625e-01 1.21430345e-01\n",
      "  4.88762558e-02]]\n",
      "1:  hazem\n",
      "2:  hazem\n",
      "Recording (12)_638396660156118051_wav.wav\n",
      "[[ 0.22220309  0.27284932  0.4684556   0.38005254 95.803474    0.22109577\n",
      "   0.12433653  1.7022731   0.8052634 ]]\n",
      "1:  hazem\n",
      "2:  hazem\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/hazem\")[0:5]:\n",
    "  testFeatures = features_extractor(f\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/other sentences/hazem/{file}\") \n",
    "  print(classifier.predict_proba(testFeatures)*100)\n",
    "  print(\"1: \",prediction(classifier.predict(testFeatures)))\n",
    "  print(\"2: \",prediction(my_predict(testFeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5854694   0.43981007  0.60611767  0.6622817   5.079853    0.25413528\n",
      "   0.36800468  3.9580405  87.04629   ]]\n",
      "1:  other\n",
      "2:  other\n"
     ]
    }
   ],
   "source": [
    "testFeatures = features_extractor(f\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/dataset/recorded/1.wav\") \n",
    "print(classifier.predict_proba(testFeatures)*100)\n",
    "print(\"1: \",prediction(classifier.predict(testFeatures)))\n",
    "print(\"2: \",prediction(my_predict(testFeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.2036552e-03 5.4373629e-03 1.5039996e-02 5.4222266e-03 1.8205790e-02\n",
      "  1.2029541e-02 3.8247136e-03 4.9267490e-03 9.9928902e+01]]\n",
      "1:  other\n",
      "2:  other\n",
      "[[7.5637153e-03 8.5005052e-03 2.6504014e-02 1.0347673e-02 3.1621933e-02\n",
      "  3.7527628e-02 7.2990181e-03 1.1022901e-02 9.9859612e+01]]\n",
      "1:  other\n",
      "2:  other\n",
      "[[5.8493060e-03 9.3041277e-03 2.2051904e-02 9.2687737e-03 2.8733334e-02\n",
      "  2.4155870e-02 6.5379799e-03 8.3235828e-03 9.9885773e+01]]\n",
      "1:  other\n",
      "2:  other\n",
      "[[1.3255341e-02 1.5598810e-02 5.5521924e-02 1.3886683e-02 9.1019481e-02\n",
      "  4.6477698e-02 1.3394030e-02 3.9849199e-02 9.9710999e+01]]\n",
      "1:  other\n",
      "2:  other\n",
      "[[4.4741090e-03 4.1721426e-03 1.6793078e-02 4.4763410e-03 2.6919436e-02\n",
      "  1.7957728e-02 4.3175342e-03 1.2911025e-02 9.9907982e+01]]\n",
      "1:  other\n",
      "2:  other\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/tamer\")[0:5]:\n",
    "  testFeatures = features_extractor(f\"C:/Users/omara/OneDrive/Desktop/Voice Signal Authentication/ML/new audios/tamer/{file}\") \n",
    "  print(classifier.predict_proba(testFeatures)*100)\n",
    "  print(\"1: \",prediction(classifier.predict(testFeatures)))\n",
    "  print(\"2: \",prediction(my_predict(testFeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"processing.pkl\", \"wb\")\n",
    "pickle.dump(classifier, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
